<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>Exploring the tidyverse, lakes, synthetic data, and tibble-SoilProfileCollections | humus.rocks - soil is life on the rocks</title>
    <link rel="stylesheet" href="/css/style.css" />
    <link rel="stylesheet" href="/css/fonts.css" />
    <img src="/images/musick.jpg">
<link href="/css/github.min.css" rel="stylesheet">

  </head>

  <body>
    <nav>
    <ul class="menu">
      
      <li><a href="/">Home</a></li>
      
      <li><a href="/about/">About</a></li>
      
      <li><a href="/categories/">Categories</a></li>
      
      <li><a href="/tags/">Tags</a></li>
      
      <li><a href="/index.xml">Subscribe</a></li>
      
    </ul>
    <hr/>
    </nav>

<div class="article-meta">
<h2><span class="title">Exploring the tidyverse, lakes, synthetic data, and tibble-SoilProfileCollections</span></h2>
<h3><span class="author">Andrew G. Brown</span>
 | <span class="date">2020/08/16</span></h3>
<p class="terms">
  
  
  Categories: 
  <a href="/categories/r">R</a> 
  <a href="/categories/tidyverse">tidyverse</a> 
  <a href="/categories/aqp">aqp</a> 
  <a href="/categories/soilprofilecollection">SoilProfileCollection</a> 
  <a href="/categories/mutate_profile">mutate_profile</a> 
  <a href="/categories/permute_profile">permute_profile</a> 
  <a href="/categories/limnology">limnology</a> 
  
  
  
  Tags: 
  <a href="/tags/demonstrations">demonstrations</a> 
  
  
</p>
</div>

<main>

<script src="/rmarkdown-libs/header-attrs/header-attrs.js"></script>


<p>In this brief tutorial, we will be using the <em>aqp</em> and <em>tidyverse</em> <strong>R</strong> packages for wrangling stratified environmental data from lake cores.</p>
<p>The sample <a href="https://fishandwhistle.net/post/2017/using-the-tidyverse-to-wrangle-core-data/">lake core</a> data from Dewey Dunnington’s tidyverse core-data-wrangling tutorial are pretty “typical” of raw, stratified environmental data that you might have kicking around on your hard drive. It also has a lot of information content despite very small (subset) size – which you will see in his tutorial and in what follows. Dewey (<a href="http://github.com/paleolimbot">paleolimbot</a> on GitHub) is prolific; he has developed great fundamental tools with R, Python and C++ and you should check out his work.</p>
<p>So, that is why we are using lake cores rather than the typical canned soil-database examples from <em>aqp</em> and <em>soilDB</em>. Lake sediments are cool and important troves of information – even where they do not technically meet our criteria for subaqueous soils! I think pedology benefits from study of (paleo-)limnology and it’s techniques!</p>
<p>Install the required packages if needed. Be sure to get the latest version of <em>aqp</em> off <a href="http://ncss.tech.github.io/aqp/">GitHub</a>. At time of writing this post (August 2020), the features that allow for support for <code>tbl_df</code> and <code>data.table</code> in <em>SoilProfileCollection</em> objects are not yet on CRAN [but will be soon].</p>
<pre class="r"><code>packages.to.install &lt;- c(&quot;aqp&quot;, &quot;tidyverse&quot;)

if (!all(packages.to.install %in% installed.packages())) {
  
   # install dependencies and CRAN version of packages
   install.packages(c(&quot;remotes&quot;, packages.to.install)) 
     
   # install development version of aqp
   remotes::install_github(&quot;ncss-tech/aqp&quot;, dependencies = FALSE, quiet = TRUE)
}</code></pre>
<p>Load the packages. We will use <em>ggplot2</em> for most plots and base <strong>R</strong> <em>graphics</em> for <em>SoilProfileCollection</em> sketches.</p>
<pre class="r"><code>library(aqp)
library(tidyverse)
library(ggplot2)</code></pre>
<pre class="r"><code># lake core data from @paleolimbot&#39;s tutorial
# https://fishandwhistle.net/post/2017/using-the-tidyverse-to-wrangle-core-data

# use tribble function to read in a comma-separated set of expressions and create a tibble
pocmaj_raw &lt;- tribble(
  ~sample_id, ~Ca, ~Ti, ~V,  
  &quot;poc15-2 0&quot;, 1036, 1337, 29,
  &quot;poc15-2 0&quot;, 1951, 2427, 31,
  &quot;poc15-2 0&quot;, 1879, 2350, 39,
  &quot;poc15-2 1&quot;, 1488, 2016, 36,
  &quot;poc15-2 2&quot;, 2416, 3270, 79,
  &quot;poc15-2 3&quot;, 2253, 3197, 79,
  &quot;poc15-2 4&quot;, 2372, 3536, 87,
  &quot;poc15-2 5&quot;, 2621, 3850, 86,
  &quot;poc15-2 5&quot;, 2785, 3939, 95,
  &quot;poc15-2 5&quot;, 2500, 3881, 80,
  &quot;maj15-1 0&quot;, 1623, 2104, 73,
  &quot;maj15-1 0&quot;, 1624, 2174, 73,
  &quot;maj15-1 0&quot;, 2407, 2831, 89,
  &quot;maj15-1 1&quot;, 1418, 2409, 70,
  &quot;maj15-1 2&quot;, 1550, 2376, 70,
  &quot;maj15-1 3&quot;, 1448, 2485, 64,
  &quot;maj15-1 4&quot;, 1247, 2414, 57,
  &quot;maj15-1 5&quot;, 1463, 1869, 78,
  &quot;maj15-1 5&quot;, 1269, 1834, 71,
  &quot;maj15-1 5&quot;, 1505, 1989, 94
)

# inspect 
head(pocmaj_raw)</code></pre>
<pre><code>## # A tibble: 6 x 4
##   sample_id    Ca    Ti     V
##   &lt;chr&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;
## 1 poc15-2 0  1036  1337    29
## 2 poc15-2 0  1951  2427    31
## 3 poc15-2 0  1879  2350    39
## 4 poc15-2 1  1488  2016    36
## 5 poc15-2 2  2416  3270    79
## 6 poc15-2 3  2253  3197    79</code></pre>
<p>On inspection we find a field <code>sample_id</code> that contains <em>site</em>, <em>year</em>, <em>core</em> and <em>layer</em>-level identifiers. Some of these combinations are repeated with unique values of analytes. In this case, assume the layer identifier is the top depth in meters AND that the cores were collected in constant increments of 1 meter. The elements are measured with a portable X-ray fluorescence unit – which does not truly measure concentration – I have reported the units as [pXRF] here.</p>
<p>For the sample data, we have repeated measures in depth – because points in lakes were sampled at successively greater depth slices. The core segments along Z collected at an X, Y location in a lake are not independent, though they may intercept layers that are dissimilar from one another. In the sample data, the shallowest and deepest layers (<code>"0"</code> and <code>"5"</code>) have replicates. If this were our data, we would try to make use of this with the rest of the cores – but we only have them for a subset of depths in two cores, so we generalize with the <code>median()</code>.</p>
<div id="dplyr-wrangling-for-input-to-aqp" class="section level2">
<h2><em>dplyr</em> wrangling for input to <em>aqp</em></h2>
<p>We take the raw data and process it for use in the <em>aqp</em> package.</p>
<p>We will use the <code>magrittr</code> “pipe” (<code>%&gt;%</code>) operator to pipe or chain-together commands. The <code>%&gt;%</code> facilitates passing variables to functions of the form <code>f(x, y)</code> as <code>x %&gt;% f(y)</code>. This is unnecessary with only a single “step,” but is particularly useful whenever you don’t want to create a bunch of intermediate variables or would like to keep the sequence of your workflow easy to read.</p>
<pre class="r"><code>pocmaj &lt;- pocmaj_raw %&gt;%
  group_by(sample_id) %&gt;%                 # group replicates together
  summarize(Ca = median(Ca), 
            Ti = median(Ti), 
            V  = median(V)) %&gt;%           # calculate sample analyte median
  separate(sample_id, sep = &quot; &quot;,
           into = c(&quot;core&quot;, &quot;depth&quot;)) %&gt;% # split sample_id -&gt; core + depth
  mutate(top = as.numeric(depth) * 100,   # calculate top and bottom depth
         bottom = top + 100)              #  (assuming 100 cm increments)</code></pre>
<p>We <code>group_by()</code> <code>sample_id</code> and then <code>summarize()</code> the <code>median()</code> for each <code>sample_id</code> and analyte. This “flattens” a possible many-to-one relationship for values per depth. After that we <code>separate()</code> <code>sample_id</code> into <code>core</code> (site-level) and <code>depth</code> (layer-level) IDs using <code>" "</code> (a space) as the delimiter. Finally, we we scale <code>depth</code> by a factor of <code>100</code> to convert from meters to centimeters.</p>
<p>We could choose <code>mean()</code> and <code>sd()</code> as they are unbiased estimators for our layer-level summary function. The demonstration shown later that uses profile-level standard deviations in Vanadium value could be relatively easily modified to use layer-level variance derived from replicates/bulk/sub samples etc. Other summary statistics could be calculated such as <code>max()</code> for risk-assessment. We use <code>median()</code> because it is good for central tendency and we will be permuting the values around a “centroid” as our final demonstration.</p>
</div>
<div id="promote-tbl_df-to-soilprofilecollection-via-the-formula-interface" class="section level2">
<h2>Promote <em>tbl_df</em> to <em>SoilProfileCollection</em> via the formula interface</h2>
<p>Once you have a site-level ID, top and bottom depth inside a <em>data.frame</em>-like object, you can use the formula interface to “promote” it to a <em>SoilProfileCollection</em> object. This provides user access to to site and layer-level fields via the <code>site()</code> and <code>horizons()</code> S4 methods, respectively.</p>
<p>For the sample data, or any stratified data without a bottom depth, we want to fill in a <code>bottom</code> depth using known sampling intervals and core geometry. This could be a bit of a “leap” if your values are actually “point” estimates and cannot be considered “representative” for the whole layer/block.</p>
<p>In <em>aqp</em>, the top and bottom depths are used as checks for depth-based ordering and topology – which some algorithms are sensitive to. <code>depths&lt;-</code> sorts layer data (in <code>@horizons</code> slot) first on profile ID (<code>core</code>) then on <code>top</code> depth. From sorted layer data, the <code>@site</code> slot data are derived.</p>
<pre class="r"><code># promote pocmaj to SoilProfileCollection 
depths(pocmaj) &lt;- core ~ top + bottom</code></pre>
<p><code>core</code>, <code>top</code> and <code>bottom</code> are all columns that exist in <code>pocmaj</code>. <em>tbl_df</em> and <em>data.table</em> are subclasses supported by the <em>SoilProfileCollection</em> in addition to <em>data.frame</em>.</p>
<div id="depths-" class="section level3">
<h3><code>depths&lt;-</code></h3>
<p>Promoting via the formula interface follows this form:</p>
<pre class="r"><code>depths(your.df) &lt;- unique_site_ID ~ topdepth + bottomdepth</code></pre>
<p><code>your.df</code> is expected to be an object that inherits from <em>data.frame</em>. The <code>unique_site_ID</code> is shared between all layers associated with a single “profile,” but there is no specific guarantee that the unique_site_ID*topdepth combination results in an layers that are in order, without overlaps, gaps, etc. You can check profile-level validity for a whole collection with the function <code>aqp::checkHzDepthLogic()</code>. Ensuring that the site and layer data have parity can be achieved with <code>aqp::spc_in_sync()</code> – though this should not be necessary for <em>most</em> external uses of <em>aqp</em>.</p>
</div>
<div id="soil-profile-sketches-with-plotspc" class="section level3">
<h3>Soil profile sketches with <code>plotSPC</code></h3>
<p>Let’s make a profile sketch of core data for a single analyte, Vanadium (<code>V</code>). We will use this analyte through the rest of the tutorial.</p>
<p><strong>From <a href="https://en.wikipedia.org/wiki/Vanadium">Wikipedia</a>:</strong></p>
<blockquote>
<p>Vanadium is a chemical element with the symbol V and atomic number 23. It is a hard, silvery-grey, malleable transition metal. The elemental metal is rarely found in nature … Vanadium occurs naturally in about 65 minerals and in fossil fuel deposits. It is produced in China and Russia from steel smelter slag. Other countries produce it either from magnetite directly, flue dust of heavy oil, or as a byproduct of uranium mining. It is mainly used to produce specialty steel alloys such as high-speed tool steels, and some aluminium alloys.</p>
</blockquote>
<pre class="r"><code># plot soil profile sketch of one variable (V = Vanadium)
plot(pocmaj, color = &quot;V&quot;, name = &quot;hzID&quot;)</code></pre>
<p><img src="/post/2020-08-16-exploring-tibble-lakes-aqp-tidyverse_files/figure-html/unnamed-chunk-7-1.png" width="672" /></p>
</div>
</div>
<div id="producing-site-level-summaries-from-layer-level-data-with-mutate_profile." class="section level2">
<h2>Producing site-level summaries from layer-level data with <code>mutate_profile</code>.</h2>
<p><code>mutate_profile</code> is a special variety of <code>mutate</code> defined by the <em>aqp</em> package. It contains built-in iteration to support multiple non-standard evaluation expressions that are evaluated in the context of each single profile in a collection.</p>
<p>The result must either resolve to unit length for a profile (a site-level field) or equal in length to the number of layers for that profile (a layer-level field).</p>
<p>Let’s define a function that we can use to summarize <em>SoilProfileCollection</em> objects over arbitrary depth intervals.</p>
<pre class="r"><code>#&#39; Calculate site-level summary stats over interval [0, max.depth]
#&#39; @name summarize_core
#&#39; @param spc A SoilProfileCollection 
#&#39; @param max.depth Vertical depth to truncate output to [0, max.depth]; default: 600;
#&#39; @return A SoilProfileCollection
summarize_core &lt;- function(spc, max.depth = 600) {
  
  # max.depth is subject to some basic constraints
  stopifnot(is.numeric(max.depth), length(max.depth) == 1, max.depth &gt; 0)
  
  # truncate cores to a common depth interval [0, max.depth]
  #   suppress aqp::glom warnings about cores with depth &lt; max.depth
  res &lt;- suppressWarnings(trunc(spc, 0, max.depth))  %&gt;% 
    
    # for the depth-weighted mean, calculate horizon thickness
    #   identical to arithmetic mean if depth interval is constant
    aqp::mutate(layer_wt = bottom - top) %&gt;%
    
    # mutate_profile performs mutation by profile (site/core)
    mutate_profile(dwt_V = weighted.mean(V, layer_wt),
                   median_V = median(V), 
                   mean_V = mean(V), 
                   sd_V = sd(V))
  return(res)
}</code></pre>
<pre class="r"><code># calculate depth-weighted average V within each core truncated to upper 200cm or 600cm
pocmaj2 &lt;- summarize_core(pocmaj, 200)
pocmaj6 &lt;- summarize_core(pocmaj, 600)

# create unique profile IDs
profile_id(pocmaj2) &lt;- paste(profile_id(pocmaj2), &quot;[0 - 2 m]&quot;)
profile_id(pocmaj6) &lt;- paste(profile_id(pocmaj6), &quot;[0 - 6 m]&quot;)</code></pre>
<p>The below code demonstrates effects of total sampling depth on aggregation of stratified layers to “point” or site-level values.</p>
<pre class="r"><code># inspect result (a site-level field calculated from all layers)
pocmaj2and6 &lt;- aqp::union(list(pocmaj2, pocmaj6))

# combined plot
plot(pocmaj2and6, color = &quot;V&quot;, name = &quot;hzID&quot;)</code></pre>
<p><img src="/post/2020-08-16-exploring-tibble-lakes-aqp-tidyverse_files/figure-html/unnamed-chunk-10-1.png" width="672" /></p>
<pre class="r"><code># inspect
site(pocmaj2and6)[,c(idname(pocmaj2and6), &quot;dwt_V&quot;, &quot;median_V&quot;, &quot;mean_V&quot;, &quot;sd_V&quot;)]</code></pre>
<pre><code>## # A tibble: 4 x 5
##   core              dwt_V median_V mean_V  sd_V
##   &lt;chr&gt;             &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;
## 1 maj15-1 [0 - 2 m]  71.5     71.5   71.5  2.12
## 2 maj15-1 [0 - 6 m]  68.7     70     68.7  7.31
## 3 poc15-2 [0 - 2 m]  33.5     33.5   33.5  3.54
## 4 poc15-2 [0 - 6 m]  66.3     79     66.3 25.7</code></pre>
<p>The summary statistics reflect what we noticed above in the profile plot and will be important later. One might be interested in aggregate properties of groups of cores summarized as above. We only have two unique cores – but that won’t stop us.</p>
<p>We can take this further and aggregate cores together into groups based on proximity, process, etc. This small two-site example dataset appears to have two different water bodies or areas where cores were collected. Presumably there are more data from there. In this case, we want to compare our 2-meter depth-truncated cores to the full 6-meter depth cores to test the effect of sampling depth on observed aggregate statistics. What are you missing by coring only to 2 rather than 6? To do this we will create a synthetic dataset.</p>
</div>
<div id="generating-synthetic-data" class="section level2">
<h2>Generating synthetic data</h2>
<p>There is obviously natural variation in stratigraphy that is variably captured by a constant sampling interval. In addition, there are sampling errors such as uncertain datum, incomplete recovery and compaction. We will generate some realizations assuming that all layer boundaries have a standard deviation in depth of <code>10</code> centimeters and that <code>V</code> concentration varies related to the original value plus Gaussian noise related to a core-level variance parameter.</p>
<pre class="r"><code># assume variation in bottom depth amounts to a standard deviation of about 10% 1m core thickness
# i.e. that normal curve centered at the boundary between 100cm layers has SD of 10cm
horizons(pocmaj2and6)$layer_sd &lt;- 10</code></pre>
<p>To simulate we use the <em>aqp</em> method <code>permute_profile</code>. We use <code>profileApply</code> to iterate over cores, and <code>aqp::union</code> to combine the <code>100</code> realizations we generate for each of the four input profiles.</p>
<pre class="r"><code># random number seed
set.seed(1234)

# iterate over cores, generate 100 realizations each, aqp::union result
bigpocmaj &lt;- aqp::union(profileApply(pocmaj2and6, function(p) {
               # use a horizon-level field to produce geometric realizations
               pp &lt;- permute_profile(p, boundary.attr = &quot;layer_sd&quot;)
               
               # naive simulation of varying V values
               #  - core-specific standard deviation rounded base10 logarithm 
               #  - value cannot be negative [snaps to zero]
               pp$V &lt;- pmax(0, pp$V + 
                              rnorm(nrow(pp), mean = 0, sd = 10^round(log10(pp$sd_V))))
               
               # create a unique profile ID
               profile_id(pp) &lt;- paste0(unique(p$core), profile_id(pp))
               return(pp)
             }))</code></pre>
<p>Note that this is a relatively naive way of simulating the property values, but also is perhaps reasonable considering the vertical granularity of observations in the Z dimension.</p>
</div>
<div id="inspecting-the-synthetic-data" class="section level2">
<h2>Inspecting the synthetic data</h2>
<pre class="r"><code># simulated 400 realizations
#  4 profiles (2 cores, 0-2m and 0-6m case), 100 times each
length(bigpocmaj)</code></pre>
<pre><code>## [1] 400</code></pre>
<pre class="r"><code># extract horizons tbl_df
h &lt;- horizons(bigpocmaj)

# denormalize site-level core variable to horizon
h$core &lt;- denormalize(bigpocmaj, &quot;core&quot;)

# create factor levels for combined plots from multiple data sources
littlebig &lt;- rbind(data.frame(pocmaj = &quot;S&quot;, h[,horizonNames(pocmaj)]),
                   data.frame(pocmaj = &quot;O&quot;,  horizons(pocmaj)))

# set up colors and labels
cbPalette &lt;- c(&quot;#009E73&quot;, &quot;#E69F00&quot;)
cbLabels &lt;- c(sprintf(&quot;Median data &amp; density (n = %s, bw = 5)&quot;, sum(!is.na(pocmaj$V))), 
              sprintf(&quot;Synthetic data (n = %s, bins = 100)&quot;, sum(!is.na(bigpocmaj$V))))

# plot
ggplot(littlebig, aes(x = V, after_stat(density),
                      color = factor(pocmaj), fill = factor(pocmaj))) + 
  geom_histogram(data = filter(littlebig, pocmaj == &quot;S&quot;), bins = 100) +
  geom_density(data = filter(littlebig, pocmaj == &quot;O&quot;), aes(x = V, fill = NULL), bw = 5) +
  geom_point(data = filter(littlebig, pocmaj == &quot;O&quot;), aes(x = V, y = 0)) +
  labs(title = &quot;Median v.s. Synthetic bigpocmaj data\nVanadium by pXRF&quot;,
       x = &quot;Vanadium (V), pXRF&quot;, y = &quot;Density&quot;, color = &quot;Legend&quot;, fill = &quot;Legend&quot;) +
  scale_fill_manual(values = cbPalette, labels = cbLabels) +
  scale_color_manual(values = cbPalette, labels = cbLabels) +
  theme_bw()</code></pre>
<p><img src="/post/2020-08-16-exploring-tibble-lakes-aqp-tidyverse_files/figure-html/unnamed-chunk-14-1.png" width="672" /></p>
<p>We see bi-modal patterns exhibited in the layer medians are reflected and fleshed out in our simulated data <code>bigpocmaj</code>. Notice the extreme values at edges of the empirical distribution – which are <em>extrapolations</em> beyond the range observed in the input data.</p>
</div>
<div id="grouped-profile-plot" class="section level2">
<h2>Grouped Profile Plot</h2>
<p>This is a <code>groupedProfilePlot()</code> showing the different realizations generated from each of the four “parent” core descriptions. They look pretty noisy as they are unaware of any sort of meaningful pattern of depth covariance.</p>
<pre class="r"><code># default layer label is &quot;name&quot;
horizons(bigpocmaj)$name &lt;- &quot;&quot;

# plot
groupedProfilePlot(bigpocmaj[sample(1:length(bigpocmaj), size = 30),], 
                   groups = &quot;core&quot;, color = &quot;V&quot;, print.id = FALSE)</code></pre>
<p><img src="/post/2020-08-16-exploring-tibble-lakes-aqp-tidyverse_files/figure-html/unnamed-chunk-15-1.png" width="672" /></p>
</div>
<div id="compare-percentiles" class="section level2">
<h2>Compare percentiles</h2>
<pre class="r"><code># raw v.s. median (analyte 1:1 with layer) v.s. synthetic
rbind(tibble(Source = &quot;Raw&quot;,       Q = t(round(quantile(pocmaj_raw$V)))),
      tibble(Source = &quot;Median&quot;,    Q = t(round(quantile(pocmaj2and6$V)))),
      tibble(Source = &quot;Synthetic&quot;, Q = t(round(quantile(bigpocmaj$V)))))</code></pre>
<pre><code>## # A tibble: 3 x 2
##   Source    Q[,&quot;0%&quot;] [,&quot;25%&quot;] [,&quot;50%&quot;] [,&quot;75%&quot;] [,&quot;100%&quot;]
##   &lt;chr&gt;        &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;
## 1 Raw             29       62       73       82        95
## 2 Median          31       52       70       78        87
## 3 Synthetic        4       48       69       77       113</code></pre>
<p>Note that the <code>median()</code>-based summary by layer “tamps” down the more extreme values from the raw data. However, the synthetic distribution captures the original range <em>and then some</em>. This is because we are “permuting” or “perturbing” Vanadium values using core-level variance parameters and assumptions of normality.</p>
<p>A maximum value of 113 pXRF is obtained in the synthetic sample – which is nearly 30 pXRF greater than the highest median, and about 20 pXRF greater than the highest observed raw value. For estimation of extrema rather than central tendency above approaches would need to be adjusted, but this is fine for this demonstration. It looks reasonably convincing when you compare to the density plot – though there really is just very little input data in this demo.</p>
<pre class="r"><code># use aqp::slab to calculate quantiles of V for 10cm &quot;slabs&quot; grouped by core
#  this is layer-level data result in a data.frame
hbpp &lt;- slab(bigpocmaj, core ~ V, slab.structure = 10)
hbpp &lt;- hbpp[complete.cases(hbpp),]

# small jitter to prevent overplotting of medians
hbpp$p.q50 &lt;- jitter(hbpp$p.q50, amount = 1)

# set up colors
cbPalette &lt;- c(&quot;#009E73&quot;, &quot;#E69F00&quot;, &quot;#cc79a7&quot;, &quot;#0072b2&quot;)

# plot
ggplot(hbpp, aes(x = V, y = top)) + 
       scale_y_reverse() +
       labs(title = &quot;Synthetic `bigpocmaj` 10cm slab-wise median V [pXRF]\n w/ 0.05, 0.95 quantiles&quot;, 
            x = &quot;Vanadium (V), pXRF&quot;, y = &quot;Depth, cm&quot;, color = &quot;Legend&quot;) +
       geom_path(data = hbpp, aes(x = p.q50, y = top, color = factor(core)), linetype = 2) +
       geom_path(data = hbpp, aes(x = p.q5, y = top, color = factor(core)), linetype = 3) + 
       geom_path(data = hbpp, aes(x = p.q95, y = top, color = factor(core)), linetype = 3) +
       scale_color_manual(values = cbPalette) + theme_bw()</code></pre>
<p><img src="/post/2020-08-16-exploring-tibble-lakes-aqp-tidyverse_files/figure-html/unnamed-chunk-17-1.png" width="672" /></p>
<!--
Here are two base **R** plots you can try on your own for comparison to _ggplot2_.


```r
plot(y = bigpocmaj$top, x = bigpocmaj$V, type="n", ylim = c(max(pretty(bigpocmaj$top)), 0))
plts <- lapply(aqp::split(bigpocmaj, f = "core"), function(bpp) {
  # calculate depth quantiles
  sbpp <- slab(bpp, ~ V, slab.structure = 10)

  # calculate color
  scol <- match(bpp$core, unique(bigpocmaj$core))
  linetype <- ifelse(max(sbpp$top[!is.na(sbpp$p.q50)]) > 201, 3, 1)

  lines(y = sbpp$top, x = sbpp$p.q5, col = scol, lwd = 1, lty = linetype)
  lines(y = sbpp$top, x = sbpp$p.q50, col = scol, lwd = 2, lty = linetype)
  lines(y = sbpp$top, x = sbpp$p.q95, col = scol, lwd = 1, lty = linetype)
})

# raw line plot
plot(y = bigpocmaj$top, x = bigpocmaj$V, type="n", ylim = c(max(pretty(bigpocmaj$top)), 0))
plts <- profileApply(bigpocmaj, function(p) {
   lines(y = p$top, x = p$V, col = match(p$core, unique(bigpocmaj$core)))
  })
```
-->
<pre class="r"><code># calculate site-level summary stats for each simulated core
simulated.groups &lt;- summarize_core(bigpocmaj, 600) %&gt;% 
  
  # group by &quot;core&quot; (original profile ID)
  aqp::group_by(core) %&gt;%
  
  # summarize profile stats by original core group
  aqp::summarize(mean_dwt_V = mean(dwt_V), 
                 mean_median_V = mean(median_V), 
                 mean_mean_V = mean(mean_V), 
                 mean_sd_V = mean(sd_V),
                 sd_dwt_V = sd(dwt_V), 
                 sd_median_V = sd(median_V), 
                 sd_mean_V = sd(mean_V), 
                 sd_sd_V = sd(sd_V))</code></pre>
<pre><code>## converting core to a factor</code></pre>
<pre class="r"><code># inspect
head(simulated.groups)</code></pre>
<pre><code>## # A tibble: 4 x 9
##   core  mean_dwt_V mean_median_V mean_mean_V mean_sd_V sd_dwt_V sd_median_V
##   &lt;chr&gt;      &lt;dbl&gt;         &lt;dbl&gt;       &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;       &lt;dbl&gt;
## 1 maj1…       71.7          71.5        71.5      2.23    0.760       0.731
## 2 maj1…       68.5          68.6        68.6     11.7     4.53        4.83 
## 3 poc1…       32.4          32.8        32.8      8.45    6.86        6.78 
## 4 poc1…       65.5          75.5        66.0     27.2     3.83        5.58 
## # … with 2 more variables: sd_mean_V &lt;dbl&gt;, sd_sd_V &lt;dbl&gt;</code></pre>
<p>So, we may not completely believe the values coming out of this naive simulation, but it shows the workflow for how you would summarize a larger amount of data.</p>
<p>Also possibly gives some ideas on how to begin comparing observed data to synthetic data. The output generally reflects the pattern we saw from the individual cores – with a little more <em>fuzziness</em>.</p>
</div>
<div id="commentary" class="section level2">
<h2>Commentary</h2>
<p>The analysis shown above can be useful for demonstrating sensitivity to factors that affect scaling and aggregation of depth-dependent properties. In these lake cores factors could include: the depth range sampled, core recovery/compaction/fall-in, variation in analytes compared to “random noise” around a single centroid, use of individual v.s. group v.s. global variance parameters, and more.</p>
<p>We assume standard deviations for core depth ranges are constant at 10 cm / 10% of a 1-meter slice. This produces a slight perturbation to the parent geometry without dramatically influencing the overall degree of aggregation. We further assume that the variation in the <em>analyte</em> can be “perturbed” for a layer based on the original value for that layer plus a Gaussian offset related to the whole-core variation in analyte.</p>
<p>With the above assumptions it is tacitly accepted that there <em>is</em> a single “mode” within a core – clearly this is not always the case in nature, especially as depth increases – but the question of whether nature can be <em>approximated</em> by a single mode will perennially be an interesting one. The above analysis is suggestive of two or more distinct modes of V value in layers: those with value ~70 pXRF, and those with about half that – but that is just based on two unrelated cores.</p>
<p>Ideally, simulation parameters would be based on <em>more than one core</em> and would be <em>depth-dependent</em>.</p>
<p><code>permute_profile</code> only deals in Gaussian offsets of boundaries which means it is limited for generating profiles that exhibit more “natural” depth-skewed distributions or long-range-order types of variation in space. That means you shouldn’t expect it to be able to generate the stuff that you might find in nature – you have to sample that yourself! And then permute it :)! That said, in the future <code>permute_profile</code> will likely be expanded to support additional distributions, parameters, models or covariance structures that can be derived from experimental/observed data.</p>
</div>
<div id="next-steps" class="section level2">
<h2>Next steps</h2>
<p>If you enjoyed this tutorial and want to expand on what you worked on here, you could try modifying the code to explore Calcium (<code>Ca</code>) or Titanium (<code>Ti</code>) values instead of Vanadium (<code>V</code>). Dewey’s <a href="https://fishandwhistle.net/post/2017/using-the-tidyverse-to-wrangle-core-data/">tutorial</a> where this dataset came from has tips on using facets to plot multiple analytes etc. Also you can adjust the layer boundary standard deviation size or number of realizations (currently <code>100</code>) per profile. You could also try and load your own datasets into this routine and modify it for your own use. I will caution you about naive assumptions of normality and independence one last time, though.</p>
<p>If discussion about identification of similar layers within depth-stratified data intrigues you, functions in <em>aqp</em> such as <code>generalize_hz</code>, <code>hzTransitionProbabilies</code>, <code>genhzTableToAdjMat</code> and <code>mostLikelyHzSequence</code> may be useful. Identifying “depth modes” in soils data is essential to properly categorizing knowledge and generating accurate interpretations of properties. Dylan Beaudette has developed several routines for visualizing generalized horizon patterns such as <code>plotSoilRelationGraph</code> and plots that show simultaneous horizon transition probabilities for multiple generalized horizons across depth.</p>
<p><a href="https://ncss-tech.github.io/stats_for_soil_survey/chapters/2_data/genhz_homework.html">Here</a> is a tutorial on initial steps for developing generalized horizon concepts for the Statistics for Soil Survey course. It needs to be expanded to demonstrate some of the graphical and modeling components.</p>
<p>The <a href="https://fishandwhistle.net/post/2018/stratigraphic-diagrams-with-tidypaleo-ggplot2/">stratigraphic diagrams with tidypaleo &amp; ggplot2</a> tutorial from Dewey has some great paleolimnology-depth-dendrograms that are getting at the same fundamental “depth mode” concepts – this might be worth exploring especially in the context of <code>aqp::slice</code> and <code>aqp::slab</code>. It also would be helpful if you are interested in expanding the plots in this tutorial to show multiple analytes.</p>
</div>

</main>

  <footer>
  <script src="//yihui.name/js/math-code.js"></script>
<script async src="//mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML"></script>
<script async src="//yihui.name/js/center-img.js"></script>

<script src="/js/highlight.min.js"></script>
<script src="/js/r.min.js"></script>
<script>
hljs.configure({languages: []});
hljs.initHighlightingOnLoad();
</script>

  
  <hr/>
  <p>Andrew G. Brown (<a href="https://humus.rocks">humus.rocks</a> | <a href="https://github.com/brownag">GitHub</a> | <a href="https://twitter.com/humus_rocks">Twitter</a>)</p>
<p><a href="https://github.com/yihui/hugo-xmin">XMin Theme by Yihui Xie</a></p>

  
  </footer>
  </body>
</html>

