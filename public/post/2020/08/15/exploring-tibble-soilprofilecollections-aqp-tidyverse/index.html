<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>Exploring lakes, synthetic data, the tibble-SoilProfileCollection and the tidyverse | humus.rocks - soil is life on the rocks</title>
    <link rel="stylesheet" href="/css/style.css" />
    <link rel="stylesheet" href="/css/fonts.css" />
    <img src="/images/musick.jpg">
<link href="/css/github.min.css" rel="stylesheet">

  </head>

  <body>
    <nav>
    <ul class="menu">
      
      <li><a href="/">Home</a></li>
      
      <li><a href="/about/">About</a></li>
      
      <li><a href="/categories/">Categories</a></li>
      
      <li><a href="/tags/">Tags</a></li>
      
      <li><a href="/index.xml">Subscribe</a></li>
      
    </ul>
    <hr/>
    </nav>

<div class="article-meta">
<h2><span class="title">Exploring lakes, synthetic data, the tibble-SoilProfileCollection and the tidyverse</span></h2>
<h3><span class="author">Andrew G. Brown</span>
 | <span class="date">2020/08/15</span></h3>
<p class="terms">
  
  
  
  
  
</p>
</div>

<main>

<script src="/rmarkdown-libs/header-attrs/header-attrs.js"></script>


<p>In this brief tutorial, we will be using the <em>aqp</em> and <em>tidyverse</em> packages for wrangling stratified environmental data from lake cores.</p>
<p>Install the required packages if needed. Be sure to get the latest version of <em>aqp</em> off <a href="http://ncss.tech.github.io/aqp/">GitHub</a>.</p>
<pre class="r"><code>install.packages(c(&quot;aqp&quot;,&quot;tidyverse&quot;))
remotes::install_github(&quot;ncss-tech/aqp&quot;, dependencies = FALSE)</code></pre>
<p>Load the packages. We will use <em>ggplot2</em> for most plots, but base <em>graphics</em> for <em>SoilProfileCollection</em> objects. There is some commented-out base <em>graphics</em> code that does essentially the same things as the <em>ggplot2</em> code for those who are interested in both.</p>
<pre class="r"><code>library(aqp)
library(tidyverse)
library(ggplot2)</code></pre>
<p>The sample <a href="https://fishandwhistle.net/post/2017/using-the-tidyverse-to-wrangle-core-data/">lake core data</a> from Dewey Dunnington (<a href="http://github.com/paleolimbot">paleolimbot</a> on GitHub) are pretty “typical” of stratified environmental data.</p>
<pre class="r"><code># lake core data from @paleolimbot&#39;s tutorial
# https://fishandwhistle.net/post/2017/using-the-tidyverse-to-wrangle-core-data

# use tribble function to read in a comma-separated set of expressions and create a tibble
pocmaj_raw &lt;- tribble(
  ~sample_id, ~Ca, ~Ti, ~V,  
  &quot;poc15-2 0&quot;, 1036, 1337, 29,
  &quot;poc15-2 0&quot;, 1951, 2427, 31,
  &quot;poc15-2 0&quot;, 1879, 2350, 39,
  &quot;poc15-2 1&quot;, 1488, 2016, 36,
  &quot;poc15-2 2&quot;, 2416, 3270, 79,
  &quot;poc15-2 3&quot;, 2253, 3197, 79,
  &quot;poc15-2 4&quot;, 2372, 3536, 87,
  &quot;poc15-2 5&quot;, 2621, 3850, 86,
  &quot;poc15-2 5&quot;, 2785, 3939, 95,
  &quot;poc15-2 5&quot;, 2500, 3881, 80,
  &quot;maj15-1 0&quot;, 1623, 2104, 73,
  &quot;maj15-1 0&quot;, 1624, 2174, 73,
  &quot;maj15-1 0&quot;, 2407, 2831, 89,
  &quot;maj15-1 1&quot;, 1418, 2409, 70,
  &quot;maj15-1 2&quot;, 1550, 2376, 70,
  &quot;maj15-1 3&quot;, 1448, 2485, 64,
  &quot;maj15-1 4&quot;, 1247, 2414, 57,
  &quot;maj15-1 5&quot;, 1463, 1869, 78,
  &quot;maj15-1 5&quot;, 1269, 1834, 71,
  &quot;maj15-1 5&quot;, 1505, 1989, 94
)

# inspect 
head(pocmaj_raw)</code></pre>
<pre><code>## # A tibble: 6 x 4
##   sample_id    Ca    Ti     V
##   &lt;chr&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;
## 1 poc15-2 0  1036  1337    29
## 2 poc15-2 0  1951  2427    31
## 3 poc15-2 0  1879  2350    39
## 4 poc15-2 1  1488  2016    36
## 5 poc15-2 2  2416  3270    79
## 6 poc15-2 3  2253  3197    79</code></pre>
<p>On inspection, there is a single field that provides insight into sampling methods and experimental design.</p>
<p>Specifically, we have <code>sample_id</code> which contains <em>site</em> (or core)-level and <em>layer</em>-level identifiers. In this case, the layer identifier is the top depth – which is convenient for field use and is used similarly in <em>aqp</em> SoilProfileCollection objects. For Dewey’s data <em>depth-dependent</em> repeated measures by layer are combined with <em>independent</em> repeated measures (replicates) within certain layers.</p>
<div id="dplyr-wrangling-for-input-to-aqp" class="section level2">
<h2><em>dplyr</em> wrangling for input to <em>aqp</em></h2>
<p>We take the raw data and process it for use in the <em>aqp</em> package. We will use the <code>magrittr</code> <code>%&gt;%</code> “pipe” operator to chain together commands that take the <em>data.frame</em>-like object as first argument.</p>
<pre class="r"><code>pond &lt;- pocmaj_raw %&gt;%
  group_by(sample_id) %&gt;%                                         # group replicates together
  summarise(Ca = median(Ca), Ti = median(Ti), V = median(V)) %&gt;%  # calculate median by sample
  separate(sample_id, into = c(&quot;core&quot;, &quot;depth&quot;), sep = &quot; &quot;) %&gt;%   # split site and layer
  mutate(top = as.numeric(depth) * 100, 
         bottom = top  + 100)                                     # assume 1m increments</code></pre>
<p>We <code>group_by()</code> <code>sample_id</code> and then <code>summarise()</code> the <code>median()</code> for each <code>sample_id</code> and analyte. This “flattens” a possible many-one relationship for values per depth. After that we <code>separate()</code> <code>sample_id</code> into <code>core</code> (site-level) and <code>depth</code> (layer-level) IDs using <code>" "</code> (a space) as the delimiter. Finally, we we scale <code>depth</code> by a factor of <code>100</code> to convert from meters to centimeters.</p>
<p>We choose <code>median()</code> as our layer-level summary function because it is less sensitive to extreme values than <code>mean()</code>. Other summary statistics could be calculated such as <code>max()</code> for risk-assessment, but not all layers have replicates, so that will not be very useful here.</p>
</div>
<div id="promote-tbl_df-to-soilprofilecollection-via-the-formula-interface" class="section level2">
<h2>Promote <em>tbl_df</em> to <em>SoilProfileCollection</em> via the formula interface</h2>
<p>Once you have a site-level variable, a top and a bottom depth inside a <em>data.frame</em>-like object, you can use the formula interface to “promote” it to a <em>SoilProfileCollection</em> object. This provides user access to to site and layer-level fields via the <code>site()</code> and <code>horizons()</code> S4 methods, respectively.</p>
<pre class="r"><code># promote pond to SoilProfileCollection and plot
depths(pond) &lt;- core ~ top + bottom</code></pre>
<p>Promoting via the formula interface follows this form <code>depths(your.df) &lt;- siteID ~ topdepth + bottomdepth</code>. <code>your.df</code> is expected to be an object that inherits from <em>data.frame</em>: specifically <em>tbl_df</em> and <em>data.table</em> are supported subclasses in addition to <em>data.frame</em>. <code>core</code>, <code>top</code> and <code>bottom</code> in this case are all columns that exist in <code>pond</code>, and likewise in your own formula, the site ID, top and bottom depths must be present in the object you are promoting to <em>SoilProfileCollection</em>.</p>
<p>Let’s do a visualization of data for a single analyte, Vanadium (<code>V</code>), that we will use through the rest of this tutorial.</p>
<pre class="r"><code># inspect soil profile sketch of one variable (V = Vanadium)
plot(pond, color = &quot;V&quot;, name = &quot;hzID&quot;)</code></pre>
<p><img src="/post/2020-08-15-exploring-tibble-enhanced-soilprofilecollections-with-the-tidyverse_files/figure-html/unnamed-chunk-6-1.png" width="672" /></p>
</div>
<div id="producing-site-level-summaries-from-layer-level-data-with-mutate_profile." class="section level2">
<h2>Producing site-level summaries from layer-level data with <code>mutate_profile</code>.</h2>
<p><code>mutate_profile</code> is a special variety of <code>mutate</code> defined by the <em>aqp</em> package. It contains built-in iteration to support multiple non-standard evaluation expressions that are evaluated in the context of each single profile in a collection. The result must either resolve to unit length for a profile (a site-level field) or equal in length to the number of layers for that profile (a layer-level field).</p>
<p>Let’s define a function that we can use to summarize arbitrary <em>SoilProfileCollection</em> objects over arbitrary depth intervals.</p>
<pre class="r"><code>summarize_core &lt;- function(p, max.depth = 600) {
  # p is a SoilProfileCollection we truncate to max.depth
  
  # max.depth is subject to some basic constraints
  stopifnot(is.numeric(max.depth), length(max.depth) == 1, max.depth &gt; 0)
  
  # truncate profiles to a common depth interval [0, max.depth]
  # suppress glom warnings about 200-600cm gap in some profiles
  
  res &lt;- suppressWarnings(trunc(p, 0, max.depth))  %&gt;% 
    # for the depth-weighted mean, calculate horizon thickness
    #  this is identical to arithmetic mean if depth interval is constant
    aqp::mutate(layer_wt = bottom - top) %&gt;%
    # mutate_profile performs mutation by profile (site/core)
    mutate_profile(dwt_V = weighted.mean(V, layer_wt),
                   median_V = median(V), mean_V = mean(V), sd_V = sd(V))
  return(res)
}</code></pre>
<pre class="r"><code># calculate depth-weighted average V within each core truncated to upper 200cm or 600cm
pond2 &lt;- summarize_core(pond, 200)
pond5 &lt;- summarize_core(pond, 600)

# create unique profile IDs
profile_id(pond2) &lt;- paste0(profile_id(pond2), &quot;_(0-2_m)&quot;)
profile_id(pond5) &lt;- paste0(profile_id(pond5), &quot;_(0-5_m)&quot;)</code></pre>
<p>The below code demonstrates effects of total sampling depth on aggregation of stratified layers to “point” or site-level values.</p>
<pre class="r"><code># inspect result (a site-level field calculated from all layers)
pond2and5 &lt;- aqp::union(list(pond2, pond5))

# combined plot
plot(pond2and5, color = &quot;V&quot;, name = &quot;hzID&quot;)</code></pre>
<p><img src="/post/2020-08-15-exploring-tibble-enhanced-soilprofilecollections-with-the-tidyverse_files/figure-html/unnamed-chunk-9-1.png" width="672" /></p>
<pre class="r"><code># inspect
site(pond2and5)[,c(idname(pond2and5),&quot;dwt_V&quot;, &quot;median_V&quot;,&quot;mean_V&quot;,&quot;sd_V&quot;)]</code></pre>
<pre><code>## # A tibble: 4 x 5
##   core            dwt_V median_V mean_V  sd_V
##   &lt;chr&gt;           &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;
## 1 maj15-1_(0-2_m)  71.5     71.5   71.5  2.12
## 2 maj15-1_(0-5_m)  68.7     70     68.7  7.31
## 3 poc15-2_(0-2_m)  33.5     33.5   33.5  3.54
## 4 poc15-2_(0-5_m)  66.3     79     66.3 25.7</code></pre>
<p>The summary statistics reflect what we noticed above in the profile plot and will be important later.</p>
<p>We can take this further and aggregate cores together into groups based on proximity, process, etc. This small two-site example dataset appears to have two different water bodies or areas where cores were collected. One might be interested in aggregate properties of groups of profiles or cores. We only have two unique profiles – but that won’t stop us.</p>
<p>In this case, we want to compare our depth-truncated profiles to the full 6-meter depth profiles to test the effect of sampling depth on observed aggregate values. To do this we will create a synthetic dataset.</p>
</div>
<div id="generating-synthetic-data" class="section level2">
<h2>Generating synthetic data</h2>
<p>We will generate some realizations assuming that all layer boundaries have a standard deviation in depth of 10 centimeters. This could feasibly arise due to sampling issues such as uncertain datum, incomplete recovery, compaction etc. in addition to natural variation in stratigraphy with respect to the continuous spatial variation in analyte concentrations and constant sampling interval.</p>
<pre class="r"><code># assume variation in bottom depth amounts to a standard deviation of about 10% 1m core thickness
# i.e. that normal curve centered at the boundary between 100cm layers has SD of 10cm
horizons(pond2and5)$layer_sd &lt;- 10</code></pre>
<p>To simulate having a bit more data, and to test the effects of depth-based block support, we use the <em>aqp</em> method <code>permute_profile</code>. We also make some simple assumptions for how the analyte varies from profile to profile. This could be expanded considerably, but we are working with very little data here.</p>
<pre class="r"><code># random number seed
set.seed(1234)

# iterate over profiles, generate 100 realizations each, aqp::union result
bigpond &lt;- aqp::union(profileApply(pond2and5, function(p) {
               # use a horizon-level field to produce geometric realizations
               pp &lt;- permute_profile(p, boundary.attr = &quot;layer_sd&quot;)
               
               # naive simulation of varying V values
               #  - gaussian offsets for V
               #  - rounded magnitude of base10 logarithm of profile-specific standard deviation
               #  - value cannot be negative [snaps to zero]
               pp$V &lt;- pmax(0, pp$V + rnorm(n = nrow(pp), mean = 0, 
                                            sd = 10^round(log10(pp$sd_V))))
               
               # create a unique profile ID
               profile_id(pp) &lt;- paste0(unique(p$core), profile_id(pp))
               return(pp)
             }))</code></pre>
</div>
<div id="inspecting-the-synthetic-data" class="section level2">
<h2>Inspecting the synthetic data</h2>
<pre class="r"><code># simulated 400 realizations
#  4 profiles (2 cores, 0-2m and 0-6m case), 100 times each
length(bigpond)</code></pre>
<pre><code>## [1] 400</code></pre>
<pre class="r"><code>library(ggplot2)

h &lt;- horizons(bigpond)
# denormalize site-level core variable to horizon
h$core &lt;- denormalize(bigpond, &quot;core&quot;)

# create factor levels for combined plots from multiple data
littlebig &lt;- rbind(data.frame(pond = &quot;Synthetic data&quot;, h[,horizonNames(pond)]),
                   data.frame(pond = &quot;Original data&quot;,  horizons(pond)))

cbPalette &lt;- c(&quot;#009E73&quot;, &quot;#E69F00&quot;)
cbLabels &lt;- c(sprintf(&quot;Original data &amp; density (n = %s, bw = 5)&quot;, sum(!is.na(pond$V))), 
              sprintf(&quot;Synthetic data (n = %s, bins = 100)&quot;, sum(!is.na(bigpond$V))))

ggplot(littlebig, aes(x = V, after_stat(density),
                      color = factor(pond), fill = factor(pond))) + 
  geom_histogram(data = filter(littlebig, pond == &quot;Synthetic data&quot;), bins = 100) +
  geom_density(data = filter(littlebig, pond == &quot;Original data&quot;), aes(x = V, fill = NULL), bw = 5) +
  geom_point(data = filter(littlebig, pond == &quot;Original data&quot;), aes(x = V, y = 0)) +
  theme_bw() + 
  labs(title = &quot;Original v.s. Synthetic bigpond data\nVanadium [ppm] concentrations in all cores and layers&quot;, 
       x = &quot;Vanadium (V), ppm&quot;, y = &quot;Density&quot;, 
       color = &quot;Legend&quot;, fill = &quot;Legend&quot;) +
  scale_fill_manual(values = cbPalette, labels = cbLabels) +
  scale_color_manual(values = cbPalette, labels = cbLabels)</code></pre>
<p><img src="/post/2020-08-15-exploring-tibble-enhanced-soilprofilecollections-with-the-tidyverse_files/figure-html/unnamed-chunk-12-1.png" width="672" /></p>
<p>We see bi-modal patterns exhibited in the original data are reflected and fleshed out in our simulated data <code>bigpond</code>. Notice the extreme values edges of the inferred distribution which are <em>extrapolations</em> beyond range observed in the input data.</p>
</div>
<div id="compare-percentiles" class="section level2">
<h2>Compare percentiles</h2>
<pre class="r"><code># original
round(quantile(pond$V))</code></pre>
<pre><code>##   0%  25%  50%  75% 100% 
##   31   62   72   79   87</code></pre>
<pre class="r"><code>#synthetic
round(quantile(bigpond$V))</code></pre>
<pre><code>##   0%  25%  50%  75% 100% 
##    4   48   69   77  113</code></pre>
<p>A maximum concentration of 114 ppm is obtained in the synthetic sample – which is nearly 30ppm greater than the highest value observed in the original data. For estimation of extrema rather than central tendency approaches may need to be adjusted.</p>
<pre class="r"><code>bpp &lt;- slab(bigpond, core ~ V, slab.structure = 25)
depths(bpp) &lt;- core ~ top + bottom

hbpp &lt;- horizons(bpp)

cbPalette &lt;- c(&quot;#009E73&quot;, &quot;#E69F00&quot;, &quot;#cc79a7&quot;, &quot;#0072b2&quot;)

ggplot(h, aes(x = V, y = top)) + 
       scale_y_reverse() +
       labs(title = &quot;Synthetic bigpond Data\nMedian Vanadium [ppm] with [0.05, 0.95] quantiles from 25 cm slabs&quot;, 
         x = &quot;Vanadium (V), ppm&quot;, y = &quot;Depth, cm&quot;, 
         color = &quot;Legend&quot;) +
      # scale_colour_manual(values = cbPalette, labels = cbLabels) + 
       geom_path(data = hbpp, aes(x = p.q50, y = top, color = factor(core)), linetype = 2) +
       geom_path(data = hbpp, aes(x = p.q5, y = top, color = factor(core)), linetype = 3) + 
       geom_path(data = hbpp, aes(x = p.q95, y = top, color = factor(core)), linetype = 3) +
       scale_color_manual(values = cbPalette)</code></pre>
<pre><code>## Warning: Removed 32 row(s) containing missing values (geom_path).

## Warning: Removed 32 row(s) containing missing values (geom_path).

## Warning: Removed 32 row(s) containing missing values (geom_path).</code></pre>
<p><img src="/post/2020-08-15-exploring-tibble-enhanced-soilprofilecollections-with-the-tidyverse_files/figure-html/unnamed-chunk-14-1.png" width="672" /></p>
<pre class="r"><code># make base plot with reversed axis
# plot(y = bigpond$top, x = bigpond$V, type=&quot;n&quot;, ylim = c(max(pretty(bigpond$top)), 0))
# plts &lt;- lapply(aqp::split(bigpond, f = &quot;core&quot;), function(bpp) {
#   # calculate depth quantiles
#   sbpp &lt;- slab(bpp, ~ V, slab.structure = 10)
# 
#   # calculate color
#   scol &lt;- match(bpp$core, unique(bigpond$core))
#   linetype &lt;- ifelse(max(sbpp$top[!is.na(sbpp$p.q50)]) &gt; 201, 3, 1)
# 
#   lines(y = sbpp$top, x = sbpp$p.q5, col = scol, lwd = 1, lty = linetype)
#   lines(y = sbpp$top, x = sbpp$p.q50, col = scol, lwd = 2, lty = linetype)
#   lines(y = sbpp$top, x = sbpp$p.q95, col = scol, lwd = 1, lty = linetype)
# })

# raw line plot
# plot(y = bigpond$top, x = bigpond$V, type=&quot;n&quot;, ylim = c(max(pretty(bigpond$top)), 0))
# plts &lt;- profileApply(bigpond, function(p) {
#    lines(y = p$top, x = p$V, col = match(p$core, unique(bigpond$core)))
#   })</code></pre>
<pre class="r"><code># calculate site-level summary stats for each simulated profile
simulated.groups &lt;- summarize_core(bigpond, 600) %&gt;% 
  # group by &quot;core&quot; (original profile ID)
  aqp::group_by(core) %&gt;%
  # summarize profile stats by original core group
  aqp::summarize(mean_dwt_V = mean(dwt_V), 
                 mean_median_V = mean(median_V), 
                 mean_mean_V = mean(mean_V), 
                 mean_sd_V = mean(sd_V),
                 sd_dwt_V = sd(dwt_V), 
                 sd_median_V = sd(median_V), 
                 sd_mean_V = sd(mean_V), 
                 sd_sd_V = sd(sd_V))</code></pre>
<pre><code>## converting core to a factor</code></pre>
<pre class="r"><code># inspect
simulated.groups</code></pre>
<pre><code>## # A tibble: 4 x 9
##   core  mean_dwt_V mean_median_V mean_mean_V mean_sd_V sd_dwt_V sd_median_V
##   &lt;chr&gt;      &lt;dbl&gt;         &lt;dbl&gt;       &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;       &lt;dbl&gt;
## 1 maj1…       71.7          71.5        71.5      2.23    0.760       0.731
## 2 maj1…       68.5          68.6        68.6     11.7     4.53        4.83 
## 3 poc1…       32.4          32.8        32.8      8.45    6.86        6.78 
## 4 poc1…       65.5          75.5        66.0     27.2     3.83        5.58 
## # … with 2 more variables: sd_mean_V &lt;dbl&gt;, sd_sd_V &lt;dbl&gt;</code></pre>
<p>So, we may not completely believe the values coming out of this naive simulation, but it shows the workflow for how you would summarize a larger amount of data and also possibly how to begin comparing it to synthetic data where you generalize process. The output generally reflects the pattern we saw from the individual profiles – with a little more <em>fuzziness</em>.</p>
</div>
<div id="commentary" class="section level2">
<h2>Commentary</h2>
<p>We assume standard deviations for core depth ranges are constant at 5cm / 5% of a 1-meter slice. Similarly, we assume that the variation in the analytes can be modeled for a particular profile’s layers based on the original value for a layer plus a gaussian offset and an exponential scaling term that is related to the profile itself.</p>
<p>There is a tacit assumption in this line of analysis that there is a single “mode” within a particular profile for the depth-distribution of each analyte that can be described relative to a known prior value for a layer, and a profile-level mean and standard deviation. Much more complex models of these relationships can be developed.</p>
<p>Ideally, simulation parameters would be based on <em>more than one profile</em> and would be <em>depth-dependent</em>. <em>permute_profile</em> only deals in Gaussian offsets of boundaries, for now. The use of <code>rnorm</code> and heuristics about mean and standard deviation of Vanadium within profiles are to just produce somewhat plausible results for this demonstration.</p>
<p>Functions like <code>aqp::generalize_hz</code> <code>aqp::hzTransitionProbabilies</code> are useful for identifying some of modes in soils data. Code like what Dewey used in his demo linked at the beginning of this tutorial to create depth-dendograms would also be useful. <a href="https://ncss-tech.github.io/stats_for_soil_survey/chapters/2_data/genhz_homework.html">Here</a> is a tutorial I developed on initial steps for developing generalized horizon models for the Statistics for Soil Survey course.</p>
<p>This type of analysis can be useful for demonstrating sensitivity to factors that affect scaling of observed properties – specifically the depth range sampled, core recovery, variation in analytes compared to “random noise” or group/global variance parameters, etc.</p>
</div>

</main>

  <footer>
  <script src="//yihui.name/js/math-code.js"></script>
<script async src="//mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML"></script>
<script async src="//yihui.name/js/center-img.js"></script>

<script src="/js/highlight.min.js"></script>
<script src="/js/r.min.js"></script>
<script>
hljs.configure({languages: []});
hljs.initHighlightingOnLoad();
</script>

  
  <hr/>
  <p>Andrew G. Brown (<a href="https://humus.rocks">humus.rocks</a> | <a href="https://github.com/brownag">GitHub</a> | <a href="https://twitter.com/humus_rocks">Twitter</a>)</p>
<p><a href="https://github.com/yihui/hugo-xmin">XMin Theme by Yihui Xie</a></p>

  
  </footer>
  </body>
</html>

